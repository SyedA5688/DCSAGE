{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_matrix_smooth',\n",
       " 'feature_matrix_unsmooth',\n",
       " 'flight_matrix_unscaled',\n",
       " 'flight_matrix_log10_scaled']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_v18 = np.load(\"./datasets/10_continents_dataset_v18_node_pert.npz\")\n",
    "dataset_v18.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_features_log10',\n",
       " 'train_log10_scaled_flight_matrix',\n",
       " 'train_unscaled_flight_matrix',\n",
       " 'val_features_log10',\n",
       " 'val_log10_scaled_flight_matrix',\n",
       " 'val_unscaled_flight_matrix',\n",
       " 'test_features_log10_unsmooth',\n",
       " 'test_features_log10_smooth',\n",
       " 'test_log10_scaled_flight_matrix',\n",
       " 'test_unscaled_flight_matrix']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_v18_train = np.load(\"/Users/syedrizvi/Desktop/Projects/GNN_Project/DCSAGE/Training-Code/datasets/10_continents_dataset_v18_training.npz\")\n",
    "dataset_v18_train.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897.0\n",
      "537.0\n",
      "815.0\n"
     ]
    }
   ],
   "source": [
    "print(dataset_v18_train[\"train_unscaled_flight_matrix\"].max())\n",
    "print(dataset_v18_train[\"val_unscaled_flight_matrix\"].max())\n",
    "print(dataset_v18_train[\"test_unscaled_flight_matrix\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define WeightedSAGEConv and DCSAGE Architectres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.norm import GraphNorm\n",
    "\n",
    "\n",
    "from typing import Union, Tuple\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "\n",
    "\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    \"\"\"The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
    "\n",
    "    Copied from torch_geometric.nn.SageConv and then modified by Sesti et. al and Juan\n",
    "    Jose Garau to take edge weights into account in message-passing step.\n",
    "\n",
    "    math:\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W_2} \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        normalize (bool, optional): If set to True, output features\n",
    "            will be l_2-normalized (default: False).\n",
    "        bias (bool, optional): If set to False, the layer will not learn\n",
    "            an additive bias. (default: True)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            torch_geometric.nn.conv.MessagePassing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                in_channels: Union[int, Tuple[int, int]],\n",
    "                out_channels: int, \n",
    "                normalize: bool = False,\n",
    "                training: bool = True,\n",
    "                root_weight = True,\n",
    "                bias: bool = True, \n",
    "                **kwargs):\n",
    "        super(WeightedSAGEConv, self).__init__(aggr='mean', **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "        self.training = training\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_weight: Tensor = None,\n",
    "                size: Size = None) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor)\n",
    "        out = self.propagate(edge_index, x=x, size=size, edge_weight=edge_weight)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out += self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2., dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i: Tensor, x_j: Tensor, edge_weight) -> Tensor:\n",
    "        \"\"\"\n",
    "        Constructs messages from node j to node i in analogy to ϕΘ for each edge in \n",
    "        edge_index. This function can take any argument as input which was initially \n",
    "        passed to propagate(). Furthermore, tensors passed to propagate() can be \n",
    "        mapped to the respective nodes i and j by appending _i or _j to the variable \n",
    "        name, .e.g. x_i and x_j.\n",
    "\n",
    "        x_i.shape and x_j.shape is [num_edges, embedding dim (num_features or graph emb dim)]\n",
    "        edge_weight.shape is [num_edges, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        return x_j * edge_weight  # [num_edges, dim] * [num_edges, 1] = [num_edges, dim]\n",
    "        # return x_j\n",
    "\n",
    "    # def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor) -> Tensor:\n",
    "    #     # Not using Sparse Tensors, so this is not called\n",
    "    #     adj_t = adj_t.set_value(None, layout=None)\n",
    "    #     return matmul(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class DynamicAdjSAGE(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                node_features: int = 3, \n",
    "                emb_dim: int = 16,\n",
    "                window_size: int = 14,\n",
    "                output: int = 1, \n",
    "                training: bool = True,\n",
    "                lstm_type: str = 'vanilla',\n",
    "                name: str = \"DASAGE\"):\n",
    "        super(DynamicAdjSAGE, self).__init__()\n",
    "        assert lstm_type in [\"vanilla\"]\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.training = training\n",
    "        self.lstm_type = lstm_type\n",
    "        self.name = name\n",
    "\n",
    "        normalize_graphsage_layers = False\n",
    "\n",
    "        self.sage1 = WeightedSAGEConv(in_channels=node_features, out_channels=self.emb_dim, normalize=normalize_graphsage_layers, training=self.training)\n",
    "        self.sage2 = WeightedSAGEConv(in_channels=self.emb_dim, out_channels=self.emb_dim, normalize=normalize_graphsage_layers, training=self.training)\n",
    "\n",
    "        self.graph_norm_1 = GraphNorm(self.emb_dim)\n",
    "        self.graph_norm_2 = GraphNorm(self.emb_dim)\n",
    "\n",
    "        self.lstm1 = nn.LSTMCell(input_size=2 * self.emb_dim, hidden_size=self.emb_dim)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=self.emb_dim, hidden_size=self.emb_dim)\n",
    "        \n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.lin1 = torch.nn.Linear(self.window_size + (2 * self.emb_dim), 13)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.lin2 = torch.nn.Linear(13, output)\n",
    "\n",
    "        # self.init_weights()  # Initialize weights with orthogonal matrices\n",
    "        \n",
    "        # For concatenating features across each day of time window\n",
    "        self.concat_feat_list = []\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.orthogonal_(self.sage1.lin_l.weight)\n",
    "        nn.init.orthogonal_(self.sage1.lin_r.weight)\n",
    "        nn.init.orthogonal_(self.sage2.lin_l.weight)\n",
    "        nn.init.orthogonal_(self.sage2.lin_r.weight)\n",
    "        # nn.init.orthogonal_(self.graph_norm_1.weight)  # Only tensors with 2+ dimensions are supported\n",
    "        # nn.init.orthogonal_(self.graph_norm_2.weight)\n",
    "\n",
    "        # Pytorch LSTMCell only has 2 weight matrices, each one is 4*hidden_size * output_size,\n",
    "        # meaning these 2 matrices contain the 8 LSTM kernels we are trying to initialize\n",
    "        nn.init.orthogonal_(self.lstm1.weight_ih)\n",
    "        nn.init.orthogonal_(self.lstm1.weight_hh)\n",
    "        nn.init.orthogonal_(self.lstm2.weight_ih)\n",
    "        nn.init.orthogonal_(self.lstm2.weight_hh)\n",
    "\n",
    "        nn.init.orthogonal_(self.lin1.weight)\n",
    "        nn.init.orthogonal_(self.lin2.weight)\n",
    "        print(\"Ran init_weights().\")\n",
    "\n",
    "    def forward(self, data: Data, h_1: Tensor=None, c_1: Tensor=None, \n",
    "                h_2: Tensor=None, c_2: Tensor=None, day_idx: int=0):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \n",
    "        graphsage_outputs = []\n",
    "        self.concat_feat_list.append(x[:,1:2])\n",
    "\n",
    "        x = self.sage1(x, edge_index, edge_attr)  # x becomes [10, self.emb_dim]\n",
    "        self.graphsage1_out = x\n",
    "        x = self.graph_norm_1(x)\n",
    "        x = F.relu(x)  \n",
    "        graphsage_outputs.append(x)\n",
    "        \n",
    "        x = self.sage2(x, edge_index, edge_attr)  # x becomes [10, self.emb_dim]\n",
    "        self.graphsage2_out = x\n",
    "        x = self.graph_norm_2(x)\n",
    "        x = F.relu(x)\n",
    "        graphsage_outputs.append(x)\n",
    "\n",
    "        x = torch.cat(graphsage_outputs, dim=1)  # x becomes [10, 2 * self.emb_dim]\n",
    "        self.graphsage_concat = x\n",
    "\n",
    "        if h_1 is None:\n",
    "            h_1 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "        if c_1 is None:\n",
    "            c_1 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "        if h_2 is None:\n",
    "            h_2 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "        if c_2 is None:\n",
    "            c_2 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "\n",
    "        h_1, c_1 = self.lstm1(x, (h_1, c_1))  # h_1 and c_1 both become [10, self.emb_dim]\n",
    "        h_2, c_2 = self.lstm2(h_1, (h_2, c_2))  # h_2 and c_2 both become [10, self.emb_dim]\n",
    "        \n",
    "        if day_idx == self.window_size - 1:\n",
    "            concat_feat = torch.cat(self.concat_feat_list, dim=1)\n",
    "            x = torch.cat((concat_feat, h_1, h_2), dim=1)\n",
    "            self.concat_feat_list.clear()\n",
    "\n",
    "            x = self.act1(x)\n",
    "            x = self.lin1(x)\n",
    "            x = self.act2(x)\n",
    "            x = self.lin2(x)\n",
    "\n",
    "        return x, h_1, c_1, h_2, c_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from covid_10country_perturb_dataset import Covid10CountriesPerturbedDataset, Covid10CountriesUnperturbedDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "TRAINING_RUN = \"2022-03-21-01_05_53\"\n",
    "MODEL_IDX = 7\n",
    "\n",
    "# Sum aggregator filtered models: [0 1 3 29 37 41 43 63 77 78 82 96]\n",
    "# Sum agg: model 10 is sensitive negatively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 10 perturbed dataloaders\n",
    "\n",
    "perturbed_dataloaders = []\n",
    "for idx in range(10):\n",
    "    dataset_unsmooth = Covid10CountriesPerturbedDataset(\n",
    "        dataset_npz_path=\"./datasets/10_continents_dataset_v18_node_pert.npz\", \n",
    "        window_size=WINDOW_SIZE, \n",
    "        data_split=\"entire-dataset-smooth\", \n",
    "        perturb_country_idx=idx, \n",
    "        avg_graph_structure=False)\n",
    "    dataloader = DataLoader(dataset_unsmooth, batch_size=800, shuffle=False)\n",
    "    perturbed_dataloaders.append(dataloader)\n",
    "\n",
    "# Define one regular unperturbed dataloaders\n",
    "unperturbed_dataset = Covid10CountriesUnperturbedDataset(\n",
    "    dataset_npz_path=\"./datasets/10_continents_dataset_v18_node_pert.npz\",\n",
    "    window_size=WINDOW_SIZE, \n",
    "    data_split=\"entire-dataset-smooth\", \n",
    "    avg_graph_structure=False)\n",
    "unperturbed_dataloader = DataLoader(unperturbed_dataset, batch_size=800, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicAdjSAGE(\n",
       "  (sage1): WeightedSAGEConv(2, 10)\n",
       "  (sage2): WeightedSAGEConv(10, 10)\n",
       "  (graph_norm_1): GraphNorm(10)\n",
       "  (graph_norm_2): GraphNorm(10)\n",
       "  (lstm1): LSTMCell(20, 10)\n",
       "  (lstm2): LSTMCell(10, 10)\n",
       "  (act1): ReLU()\n",
       "  (lin1): Linear(in_features=27, out_features=13, bias=True)\n",
       "  (act2): ReLU()\n",
       "  (lin2): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DynamicAdjSAGE(\n",
    "        node_features=2, \n",
    "        emb_dim=10, \n",
    "        window_size=WINDOW_SIZE, \n",
    "        output=1, \n",
    "        training=True, \n",
    "        lstm_type=\"vanilla\", \n",
    "        name=\"DCSAGE\")\n",
    "\n",
    "# If not running on Syed's laptop, then need to change this path to directory where 100 DCSAGE 14-day models are stored\n",
    "checkpoint = torch.load(\"/Users/syedrizvi/Desktop/Projects/GNN_Project/DCSAGE/Training-Code/training-runs-multiple-models/\" + TRAINING_RUN + \"/model_\" + str(MODEL_IDX) + \".pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Activation Plot Showing Each Day in 7-Day Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* 1 model, 512 windows in dataset, 7 days in each window\n",
    "* 1 day graph: [10, 10]  -  10 countries, embedding dim 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n"
     ]
    }
   ],
   "source": [
    "print(len(unperturbed_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get unperturbed LSTM and graphsage activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_h1_activations_list = [[] for _ in range(WINDOW_SIZE)]\n",
    "lstm_h2_activations_list = [[] for _ in range(WINDOW_SIZE)]\n",
    "\n",
    "gs_1_activations_list = [[] for _ in range(WINDOW_SIZE)]\n",
    "gs_2_activations_list = [[] for _ in range(WINDOW_SIZE)]\n",
    "gs_concat_activations_list = [[] for _ in range(WINDOW_SIZE)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_window_node_feat, batch_window_edge_idx, batch_window_edge_attr, batch_window_labels in unperturbed_dataloader:\n",
    "        for count, window_idx in enumerate(range(len(batch_window_node_feat))):\n",
    "            window_node_feat = batch_window_node_feat[window_idx]\n",
    "            window_edge_idx = batch_window_edge_idx[window_idx]\n",
    "            window_edge_attr = batch_window_edge_attr[window_idx]\n",
    "            window_labels = batch_window_labels[window_idx]\n",
    "            \n",
    "            h_1, c_1, h_2, c_2 = None, None, None, None\n",
    "            for day_idx in range(len(window_node_feat)):\n",
    "                day_node_feat = window_node_feat[day_idx]\n",
    "                day_edge_idx = window_edge_idx[day_idx]\n",
    "                day_edge_attr = window_edge_attr[day_idx]\n",
    "\n",
    "                cutoff_idx = day_edge_idx[0].tolist().index(-1)\n",
    "                day_edge_idx = day_edge_idx[:, :cutoff_idx]\n",
    "                day_edge_attr = day_edge_attr[:cutoff_idx, :]\n",
    "                \n",
    "                day_graph = Data(x=day_node_feat, edge_index=day_edge_idx, edge_attr=day_edge_attr)\n",
    "                y_hat, h_1, c_1, h_2, c_2 = model(day_graph, h_1, c_1, h_2, c_2, day_idx)\n",
    "            \n",
    "                # Accumulate LSTM and graphsage outputs for each window\n",
    "                lstm_h1_activations_list[day_idx].append(h_1.unsqueeze(0))  # h_1 is [10, 10]\n",
    "                lstm_h2_activations_list[day_idx].append(h_2.unsqueeze(0))\n",
    "                \n",
    "                gs_1_activations_list[day_idx].append(model.graphsage1_out.unsqueeze(0))\n",
    "                gs_2_activations_list[day_idx].append(model.graphsage2_out.unsqueeze(0))\n",
    "                gs_concat_activations_list[day_idx].append(model.graphsage_concat.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 560, 10, 10)\n",
      "(7, 560, 10, 10)\n",
      "(7, 560, 10, 10)\n",
      "(7, 560, 10, 10)\n",
      "(7, 560, 10, 20)\n"
     ]
    }
   ],
   "source": [
    "lstm_h1_activations = [torch.cat(activations, dim=0).numpy() for activations in lstm_h1_activations_list]\n",
    "lstm_h2_activations = [torch.cat(activations, dim=0).numpy() for activations in lstm_h2_activations_list]\n",
    "gs_1_activations = [torch.cat(activations, dim=0).numpy() for activations in gs_1_activations_list]\n",
    "gs_2_activations = [torch.cat(activations, dim=0).numpy() for activations in gs_2_activations_list]\n",
    "gs_concat_activations = [torch.cat(activations, dim=0).numpy() for activations in gs_concat_activations_list]\n",
    "\n",
    "unpert_lstm_h1_activations = np.array(lstm_h1_activations)\n",
    "unpert_lstm_h2_activations = np.array(lstm_h2_activations)\n",
    "unpert_gs_1_activations = np.array(gs_1_activations)\n",
    "unpert_gs_2_activations = np.array(gs_2_activations)\n",
    "unpert_gs_concat_activations = np.array(gs_concat_activations)\n",
    "\n",
    "print(unpert_lstm_h1_activations.shape)\n",
    "print(unpert_lstm_h2_activations.shape)\n",
    "print(unpert_gs_1_activations.shape)\n",
    "print(unpert_gs_2_activations.shape)\n",
    "print(unpert_gs_concat_activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get perturbed LSTM activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_lstm_h1_act = []\n",
    "pert_lstm_h2_act = []\n",
    "pert_gs_1_act = []\n",
    "pert_gs_2_act = []\n",
    "pert_gs_concat_act = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, dataloader in enumerate(perturbed_dataloaders):\n",
    "        lstm_h1_act = [[] for _ in range(WINDOW_SIZE)]\n",
    "        lstm_h2_act = [[] for _ in range(WINDOW_SIZE)]\n",
    "\n",
    "        gs_1_act = [[] for _ in range(WINDOW_SIZE)]\n",
    "        gs_2_act = [[] for _ in range(WINDOW_SIZE)]\n",
    "        gs_concat_act = [[] for _ in range(WINDOW_SIZE)]\n",
    "\n",
    "        for batch_window_node_feat, batch_window_edge_idx, batch_window_edge_attr, batch_window_labels in dataloader:\n",
    "            for count, window_idx in enumerate(range(len(batch_window_node_feat))):\n",
    "                window_node_feat = batch_window_node_feat[window_idx]\n",
    "                window_edge_idx = batch_window_edge_idx[window_idx]\n",
    "                window_edge_attr = batch_window_edge_attr[window_idx]\n",
    "                window_labels = batch_window_labels[window_idx]\n",
    "                \n",
    "                h_1, c_1, h_2, c_2 = None, None, None, None\n",
    "                for day_idx in range(len(window_node_feat)):\n",
    "                    day_node_feat = window_node_feat[day_idx]\n",
    "                    day_edge_idx = window_edge_idx[day_idx]\n",
    "                    day_edge_attr = window_edge_attr[day_idx]\n",
    "\n",
    "                    cutoff_idx = day_edge_idx[0].tolist().index(-1)\n",
    "                    day_edge_idx = day_edge_idx[:, :cutoff_idx]\n",
    "                    day_edge_attr = day_edge_attr[:cutoff_idx, :]\n",
    "                    \n",
    "                    day_graph = Data(x=day_node_feat, edge_index=day_edge_idx, edge_attr=day_edge_attr)\n",
    "                    y_hat, h_1, c_1, h_2, c_2 = model(day_graph, h_1, c_1, h_2, c_2, day_idx)\n",
    "                \n",
    "                    # Accumulate LSTM outputs for each window\n",
    "                    lstm_h1_act[day_idx].append(h_1.unsqueeze(0))  # h_1 is [10, 10]\n",
    "                    lstm_h2_act[day_idx].append(h_2.unsqueeze(0))\n",
    "                    \n",
    "                    gs_1_act[day_idx].append(model.graphsage1_out.unsqueeze(0))\n",
    "                    gs_2_act[day_idx].append(model.graphsage2_out.unsqueeze(0))\n",
    "                    gs_concat_act[day_idx].append(model.graphsage_concat.unsqueeze(0))\n",
    "        \n",
    "        lstm_h1_act = [torch.cat(activations, dim=0).numpy() for activations in lstm_h1_act]\n",
    "        lstm_h2_act = [torch.cat(activations, dim=0).numpy() for activations in lstm_h2_act]\n",
    "        gs_1_act = [torch.cat(activations, dim=0).numpy() for activations in gs_1_act]\n",
    "        gs_2_act = [torch.cat(activations, dim=0).numpy() for activations in gs_2_act]\n",
    "        gs_concat_act = [torch.cat(activations, dim=0).numpy() for activations in gs_concat_act]\n",
    "        \n",
    "        lstm_h1_act = np.array(lstm_h1_act)\n",
    "        lstm_h2_act = np.array(lstm_h2_act)\n",
    "        gs_1_act = np.array(gs_1_act)\n",
    "        gs_2_act = np.array(gs_2_act)\n",
    "        gs_concat_act = np.array(gs_concat_act)\n",
    "\n",
    "        pert_lstm_h1_act.append(lstm_h1_act)\n",
    "        pert_lstm_h2_act.append(lstm_h2_act)\n",
    "        pert_gs_1_act.append(gs_1_act)\n",
    "        pert_gs_2_act.append(gs_2_act)\n",
    "        pert_gs_concat_act.append(gs_concat_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 560, 10, 10)\n",
      "(10, 7, 560, 10, 10)\n",
      "(10, 7, 560, 10, 10)\n",
      "(10, 7, 560, 10, 10)\n",
      "(10, 7, 560, 10, 20)\n"
     ]
    }
   ],
   "source": [
    "pert_lstm_h1_act = np.array(pert_lstm_h1_act)\n",
    "pert_lstm_h2_act = np.array(pert_lstm_h2_act)\n",
    "pert_gs_1_act = np.array(pert_gs_1_act)\n",
    "pert_gs_2_act = np.array(pert_gs_2_act)\n",
    "pert_gs_concat_act = np.array(pert_gs_concat_act)\n",
    "\n",
    "print(pert_lstm_h1_act.shape)\n",
    "print(pert_lstm_h2_act.shape)\n",
    "print(pert_gs_1_act.shape)\n",
    "print(pert_gs_2_act.shape)\n",
    "print(pert_gs_concat_act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lstmcell_activation_per_window_day(unpert_lstm_activations, pert_lstm_activations, lstm_cell_num):\n",
    "    \"\"\"\n",
    "    This function will create a 11x7 figure, where each subplot will be a distribution of the model's LSTM \n",
    "    cell activations on one day in the window\n",
    "    Args:\n",
    "        - unpert_lstm_activations: Numpy array of shape [7, 512, 10, 10] - 7 is window size, 512 windows in \n",
    "            datasets, 10 countries, 10 embedding dimension\n",
    "        - pert_lstm_activations: Numpy array of shape [10, 7, 512, 10, 10] - 10 perturbed countries, ...\n",
    "    \"\"\"\n",
    "    continents = [\"Africa\", \"North America\", \"South America\", \"Oceania\", \"Eastern Europe\", \"Western Europe\", \"Middle East\", \"South Asia\", \"Southeast-East Asia\", \"Central Asia\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=11, ncols=WINDOW_SIZE, figsize=(50,40))\n",
    "    fig.suptitle(\"DCSAGE Model {} LSTM Cell {} Activations Per Day of Window\".format(MODEL_IDX, lstm_cell_num), fontsize= 30)\n",
    "\n",
    "    for row_idx, row in enumerate(ax):\n",
    "        for col_idx, col in enumerate(row):\n",
    "            # col_idx is index of day in 7-day window\n",
    "            if row_idx == 0:\n",
    "                activation_values = list(unpert_lstm_activations[col_idx].flatten())\n",
    "            else:\n",
    "                activation_values = list(pert_lstm_activations[row_idx - 1][col_idx].flatten())\n",
    "            \n",
    "            visual_df = pd.DataFrame({\n",
    "                \"Flattened Activation Values\": activation_values,\n",
    "            })\n",
    "\n",
    "            sns.histplot(ax=col, x='Flattened Activation Values', data=visual_df, kde=True)\n",
    "            if row_idx == 0:\n",
    "                mean = unpert_lstm_activations[col_idx].mean()\n",
    "                median = np.median(unpert_lstm_activations[col_idx])\n",
    "                stddev = unpert_lstm_activations[col_idx].std()\n",
    "                subplot_title = \"(Unpert) Day {} (Mean: {:.2f}, Median: {:.2f}, Std: {:.2f})\".format(col_idx, mean, median, stddev)\n",
    "            else:\n",
    "                mean = pert_lstm_activations[row_idx - 1][col_idx].mean()\n",
    "                median = np.median(pert_lstm_activations[row_idx - 1][col_idx])\n",
    "                stddev = pert_lstm_activations[row_idx - 1][col_idx].std()\n",
    "                subplot_title = \"({} Pert) Day {} \\n(Mean: {:.2f}, Median: {:.2f}, Std: {:.2f})\".format(continents[row_idx - 1], col_idx, mean, median, stddev)\n",
    "\n",
    "            col.set_title(subplot_title)\n",
    "            col.set_xlim([-1, 1])\n",
    "\n",
    "    filename = \"model_{}_lstmcell_{}_activ_per_window_day\".format(MODEL_IDX, lstm_cell_num)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"./\" + filename + '.png', bbox_inches='tight', facecolor=\"white\")\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lstmcell_activation_per_window_day(unpert_lstm_h1_activations, pert_lstm_h1_act, lstm_cell_num=1)\n",
    "plot_lstmcell_activation_per_window_day(unpert_lstm_h2_activations, pert_lstm_h2_act, lstm_cell_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot GraphSAGE activation distributions unperturbed vs perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphsage_activation_distrib_unpert_vs_pert(unpert_lstm_activations, pert_lstm_activations, gs_layer_id, x_bound:int, y_bound:int):\n",
    "    \"\"\"\n",
    "    This function will create a 7x1 figure, where each subplot will be activation vs timestep (day_idx) for\n",
    "    one of the embedding dimension positions in the LSTM cell.\n",
    "\n",
    "    *Important node: This function will just plot 1st day of each window. This will give 512 separate days, rather\n",
    "    than plotting windows that overlap days.\n",
    "\n",
    "    Args:\n",
    "        - unpert_lstm_activations: Numpy array of shape [7, 512, 10, 10] - 7 is window size, 512 windows in \n",
    "            datasets, 10 countries, 10 embedding dimension\n",
    "        - pert_lstm_activations: Numpy array of shape [10, 7, 512, 10, 10] - 10 perturbed countries, ...\n",
    "    \"\"\"\n",
    "    continents = [\"Africa\", \"North America\", \"South America\", \"Oceania\", \"Eastern Europe\", \"Western Europe\", \"Middle East\", \"South Asia\", \"Southeast-East Asia\", \"Central Asia\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=11, ncols=10, figsize=(50,60))\n",
    "    fig.suptitle(\"DCSAGE Model {} GraphSAGE Layer {} Activations\".format(MODEL_IDX, gs_layer_id), fontsize= 30)\n",
    "\n",
    "    for row_idx, row in enumerate(ax):\n",
    "        for col_idx, col in enumerate(row):\n",
    "            # col_idx is index of the country being affected by perturbation\n",
    "            if col_idx == row_idx - 1:\n",
    "                continue  # Don't plot a country when it is the one being perturbed\n",
    "\n",
    "            if row_idx == 0:\n",
    "                activation_values = list(unpert_lstm_activations[0, :, col_idx, :].flatten())\n",
    "            else:\n",
    "                activation_values = list(pert_lstm_activations[row_idx - 1, 0, :, col_idx, :].flatten())\n",
    "            \n",
    "            visual_df = pd.DataFrame({\n",
    "                \"Flattened Activation Values\": activation_values,\n",
    "            })\n",
    "\n",
    "            sns.histplot(ax=col, x='Flattened Activation Values', data=visual_df, kde=True)\n",
    "            if row_idx == 0:\n",
    "                mean = unpert_lstm_activations[0, :, col_idx, :].mean()\n",
    "                median = np.median(unpert_lstm_activations[0, :, col_idx, :])\n",
    "                stddev = unpert_lstm_activations[0, :, col_idx, :].std()\n",
    "                subplot_title = \"(Unpert) {} Distrib \\n(Mean: {:.2f}, Median: {:.2f}, Std: {:.2f})\".format(continents[col_idx], mean, median, stddev)\n",
    "            else:\n",
    "                mean = pert_lstm_activations[row_idx - 1, 0, :, col_idx, :].mean()\n",
    "                median = np.median(pert_lstm_activations[row_idx - 1, 0, :, col_idx, :])\n",
    "                stddev = pert_lstm_activations[row_idx - 1, 0, :, col_idx, :].std()\n",
    "                subplot_title = \"({} Pert) {} Distrib \\n(Mean: {:.2f}, Median: {:.2f}, Std: {:.2f})\".format(continents[row_idx - 1], continents[col_idx], mean, median, stddev)\n",
    "\n",
    "            col.set_title(subplot_title)\n",
    "            col.set_xlim([-1 * x_bound, x_bound])\n",
    "            col.set_ylim([0, y_bound])\n",
    "\n",
    "    filename = \"model_{}_gslayer_{}_activ\".format(MODEL_IDX, gs_layer_id)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"./\" + filename + '.png', bbox_inches='tight', facecolor=\"white\")\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphsage_activation_distrib_unpert_vs_pert(unpert_gs_1_activations, pert_gs_1_act, \"1\", 10, 700)\n",
    "plot_graphsage_activation_distrib_unpert_vs_pert(unpert_gs_2_activations, pert_gs_2_act, \"2\", 2, 700)\n",
    "plot_graphsage_activation_distrib_unpert_vs_pert(unpert_gs_concat_activations, pert_gs_concat_act, \"concat\", 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot GraphSAGE TSNE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [\"Brazil\", \"Germany\", \"Spain\", \"France\", \"Britain\", \"India\", \"Italy\", \"Russia\", \"Turkey\", \"USA\"]\n",
    "\n",
    "def plot_graphsage_embedding(one_day_emb, title, savename):\n",
    "    assert one_day_emb.shape == torch.Size([10, 10]), \"Unrecognized shape\"\n",
    "    reduced_graphsage_emb = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\").fit_transform(one_day_emb)\n",
    "    # print(reduced_graphsage_emb)\n",
    "    \n",
    "    fig = sns.scatterplot(x=reduced_graphsage_emb[:,0], y=reduced_graphsage_emb[:,1])\n",
    "    for i in range(10):\n",
    "        fig.text(x=reduced_graphsage_emb[i,0] + 0.2, y=reduced_graphsage_emb[i,1] + 0.5, s=countries[i])\n",
    "        # print(reduced_graphsage_emb[i,0] + 0.2, reduced_graphsage_emb[i,1] + 0.5)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Recuced Dimension X\")\n",
    "    plt.ylabel(\"Recuced Dimension Y\")\n",
    "    plt.savefig(savename, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_window_node_feat, batch_window_edge_idx, batch_window_edge_attr, batch_window_labels in unperturbed_dataloader:\n",
    "        for window_idx in range(500, 510):\n",
    "            window_node_feat = batch_window_node_feat[window_idx]\n",
    "            window_edge_idx = batch_window_edge_idx[window_idx]\n",
    "            window_edge_attr = batch_window_edge_attr[window_idx]\n",
    "            window_labels = batch_window_labels[window_idx]\n",
    "\n",
    "            h_1, c_1, h_2, c_2 = None, None, None, None\n",
    "            day_idx = 0\n",
    "            day_node_feat = window_node_feat[day_idx]\n",
    "            day_edge_idx = window_edge_idx[day_idx]\n",
    "            day_edge_attr = window_edge_attr[day_idx]\n",
    "\n",
    "            cutoff_idx = day_edge_idx[0].tolist().index(-1)\n",
    "            day_edge_idx = day_edge_idx[:, :cutoff_idx]\n",
    "            day_edge_attr = day_edge_attr[:cutoff_idx, :]\n",
    "            \n",
    "            day_graph = Data(x=day_node_feat, edge_index=day_edge_idx, edge_attr=day_edge_attr)\n",
    "            y_hat, h_1, c_1, h_2, c_2 = model(day_graph, h_1, c_1, h_2, c_2, day_idx)\n",
    "\n",
    "            plot_graphsage_embedding(model.graphsage1_out, \n",
    "                title=\"GraphSAGE 1 Embedding Window {} Model {}\".format(window_idx, MODEL_IDX), \n",
    "                savename=\"win{}_model{}_graphsage_1_emb.png\".format(window_idx, MODEL_IDX))\n",
    "            plot_graphsage_embedding(model.graphsage2_out, \n",
    "                title=\"GraphSAGE 2 Embedding Window {} Model {}\".format(window_idx, MODEL_IDX), \n",
    "                savename=\"win{}_model{}_graphsage_2_emb.png\".format(window_idx, MODEL_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphsage_embedding_10days(one_day_emb_list, title, savename):\n",
    "    reduced_graphsage_emb_list = []\n",
    "    for i in range(len(one_day_emb_list)):\n",
    "        reduced_graphsage_emb = TSNE(n_components=2, learning_rate=\"auto\", init=\"random\").fit_transform(one_day_emb_list[i])\n",
    "        reduced_graphsage_emb_list.append(reduced_graphsage_emb)\n",
    "    \n",
    "    country_x = []\n",
    "    country_y = []\n",
    "    country_name = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        country_x += [reduced_emb[i,0] for reduced_emb in reduced_graphsage_emb_list]\n",
    "        country_y += [reduced_emb[i,1] for reduced_emb in reduced_graphsage_emb_list]\n",
    "        country_name += [countries[i]] * len(reduced_graphsage_emb_list)\n",
    "\n",
    "    visual_df = pd.DataFrame({\n",
    "        \"Reduced Dimension X\": country_x,\n",
    "        \"Reduced Dimension Y\": country_y,\n",
    "        \"Country\": country_name\n",
    "    })\n",
    "\n",
    "    sns.set_theme()\n",
    "    sns.scatterplot(data=visual_df, x=\"Reduced Dimension X\", y=\"Reduced Dimension Y\", hue=\"Country\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Recuced Dimension X\")\n",
    "    plt.ylabel(\"Recuced Dimension Y\")\n",
    "    plt.savefig(savename, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_window_node_feat, batch_window_edge_idx, batch_window_edge_attr, batch_window_labels in unperturbed_dataloader:\n",
    "        graphsage1_list = []\n",
    "        graphsage2_list = []\n",
    "        for window_idx in range(410, 510):\n",
    "            window_node_feat = batch_window_node_feat[window_idx]\n",
    "            window_edge_idx = batch_window_edge_idx[window_idx]\n",
    "            window_edge_attr = batch_window_edge_attr[window_idx]\n",
    "            window_labels = batch_window_labels[window_idx]\n",
    "\n",
    "            h_1, c_1, h_2, c_2 = None, None, None, None\n",
    "            day_idx = 0\n",
    "            day_node_feat = window_node_feat[day_idx]\n",
    "            day_edge_idx = window_edge_idx[day_idx]\n",
    "            day_edge_attr = window_edge_attr[day_idx]\n",
    "\n",
    "            cutoff_idx = day_edge_idx[0].tolist().index(-1)\n",
    "            day_edge_idx = day_edge_idx[:, :cutoff_idx]\n",
    "            day_edge_attr = day_edge_attr[:cutoff_idx, :]\n",
    "            \n",
    "            day_graph = Data(x=day_node_feat, edge_index=day_edge_idx, edge_attr=day_edge_attr)\n",
    "            y_hat, h_1, c_1, h_2, c_2 = model(day_graph, h_1, c_1, h_2, c_2, day_idx)\n",
    "\n",
    "            graphsage1_list.append(model.graphsage1_out)\n",
    "            graphsage2_list.append(model.graphsage2_out)\n",
    "\n",
    "        plot_graphsage_embedding_10days(graphsage1_list, \n",
    "            title=\"GraphSAGE 1 Embedding Windows 410-510 Model {}\".format(MODEL_IDX), \n",
    "            savename=\"100days410-510_model{}_graphsage_1_emb.png\".format(MODEL_IDX))\n",
    "        plot_graphsage_embedding_10days(graphsage2_list, \n",
    "            title=\"GraphSAGE 2 Embedding Window 410-510 Model {}\".format(MODEL_IDX), \n",
    "            savename=\"100days410-510_model{}_graphsage_2_emb.png\".format(MODEL_IDX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43c3ec5cb0d81e7b9f9908a53ca28aa4318265e5d52f388cac911a9765dd2a07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
