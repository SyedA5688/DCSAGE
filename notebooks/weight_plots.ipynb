{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.norm import GraphNorm\n",
    "\n",
    "from typing import Union, Tuple\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 7\n",
    "TRAINING_RUN = \"2022-03-21-01_05_53\"\n",
    "MODEL_IDX = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSAGEConv(MessagePassing):\n",
    "    \"\"\"The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
    "\n",
    "    Copied from torch_geometric.nn.SageConv and then modified by Sesti et. al and Juan\n",
    "    Jose Garau to take edge weights into account in message-passing step.\n",
    "\n",
    "    math:\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W_2} \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        normalize (bool, optional): If set to True, output features\n",
    "            will be l_2-normalized (default: False).\n",
    "        bias (bool, optional): If set to False, the layer will not learn\n",
    "            an additive bias. (default: True)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            torch_geometric.nn.conv.MessagePassing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                in_channels: Union[int, Tuple[int, int]],\n",
    "                out_channels: int, \n",
    "                normalize: bool = False,\n",
    "                training: bool = True,\n",
    "                root_weight = True,\n",
    "                bias: bool = True, \n",
    "                **kwargs):\n",
    "        super(WeightedSAGEConv, self).__init__(aggr='mean', **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "        self.training = training\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, edge_weight: Tensor = None,\n",
    "                size: Size = None) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor)\n",
    "        out = self.propagate(edge_index, x=x, size=size, edge_weight=edge_weight)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out += self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2., dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_i: Tensor, x_j: Tensor, edge_weight) -> Tensor:\n",
    "        \"\"\"\n",
    "        Constructs messages from node j to node i in analogy to ϕΘ for each edge in \n",
    "        edge_index. This function can take any argument as input which was initially \n",
    "        passed to propagate(). Furthermore, tensors passed to propagate() can be \n",
    "        mapped to the respective nodes i and j by appending _i or _j to the variable \n",
    "        name, .e.g. x_i and x_j.\n",
    "\n",
    "        x_i.shape and x_j.shape is [num_edges, embedding dim (num_features or graph emb dim)]\n",
    "        edge_weight.shape is [num_edges, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        return x_j * edge_weight  # [num_edges, dim] * [num_edges, 1] = [num_edges, dim]\n",
    "        # return x_j\n",
    "\n",
    "    # def message_and_aggregate(self, adj_t: SparseTensor, x: OptPairTensor) -> Tensor:\n",
    "    #     # Not using Sparse Tensors, so this is not called\n",
    "    #     adj_t = adj_t.set_value(None, layout=None)\n",
    "    #     return matmul(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class DynamicAdjSAGE(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Architecture\n",
    "    1 Weighted GraphSAGE layer -> 1 Weigted GraphSAGE layer -> concat outputs of both GraphSAGE layers \n",
    "        -> 1 LSTM cell -> concat LSTM cell hidden state output for N+1 day and original input features\n",
    "        X for N days -> MLP with ReLU activation.\n",
    "\n",
    "    Architecture expects to receive 1 day graphs, so that adjacency matrix can change every day. N \n",
    "    1-day graphs passed through, then take model output for N+1 day. \n",
    "\n",
    "    Constructor Arguments:\n",
    "        node_features: number of features each node in 1-day graph contains\n",
    "        emd_dim: embedding dimension that WeightedGraphSAGE layers will output\n",
    "        window_size: number of 1-day graphs that will be passed through network before predicting next day\n",
    "        output: number of features to predict for each node on N+1 day\n",
    "        training: whether the model is being loaded for training or test. Affects things like dropout if present.\n",
    "        lstm_type: what type of LSTM to use ([\"vanila\"])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                node_features: int = 3, \n",
    "                emb_dim: int = 16,\n",
    "                window_size: int = 14,\n",
    "                output: int = 1, \n",
    "                training: bool = True,\n",
    "                lstm_type: str = 'vanilla',\n",
    "                name: str = \"DASAGE\"):\n",
    "        super(DynamicAdjSAGE, self).__init__()\n",
    "        assert lstm_type in [\"vanilla\"]\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.window_size = window_size\n",
    "        self.training = training\n",
    "        self.lstm_type = lstm_type\n",
    "        self.name = name\n",
    "\n",
    "        normalize_graphsage_layers = False\n",
    "\n",
    "        self.sage1 = WeightedSAGEConv(in_channels=node_features, out_channels=self.emb_dim, normalize=normalize_graphsage_layers, training=self.training)\n",
    "        self.sage2 = WeightedSAGEConv(in_channels=self.emb_dim, out_channels=self.emb_dim, normalize=normalize_graphsage_layers, training=self.training)\n",
    "\n",
    "        self.graph_norm_1 = GraphNorm(self.emb_dim)\n",
    "        self.graph_norm_2 = GraphNorm(self.emb_dim)\n",
    "\n",
    "        self.lstm1 = nn.LSTMCell(input_size=2 * self.emb_dim, hidden_size=self.emb_dim)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=self.emb_dim, hidden_size=self.emb_dim)\n",
    "        \n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.lin1 = torch.nn.Linear(self.window_size + (2 * self.emb_dim), 13)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.lin2 = torch.nn.Linear(13, output)\n",
    "\n",
    "        # self.init_weights()  # Initialize weights with orthogonal matrices\n",
    "        \n",
    "        # For concatenating features across each day of time window\n",
    "        self.concat_feat_list = []\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.orthogonal_(self.sage1.lin_l.weight)\n",
    "        nn.init.orthogonal_(self.sage1.lin_r.weight)\n",
    "        nn.init.orthogonal_(self.sage2.lin_l.weight)\n",
    "        nn.init.orthogonal_(self.sage2.lin_r.weight)\n",
    "        # nn.init.orthogonal_(self.graph_norm_1.weight)  # Only tensors with 2+ dimensions are supported\n",
    "        # nn.init.orthogonal_(self.graph_norm_2.weight)\n",
    "\n",
    "        # Pytorch LSTMCell only has 2 weight matrices, each one is 4*hidden_size * output_size,\n",
    "        # meaning these 2 matrices contain the 8 LSTM kernels we are trying to initialize\n",
    "        nn.init.orthogonal_(self.lstm1.weight_ih)\n",
    "        nn.init.orthogonal_(self.lstm1.weight_hh)\n",
    "        nn.init.orthogonal_(self.lstm2.weight_ih)\n",
    "        nn.init.orthogonal_(self.lstm2.weight_hh)\n",
    "\n",
    "        nn.init.orthogonal_(self.lin1.weight)\n",
    "        nn.init.orthogonal_(self.lin2.weight)\n",
    "        print(\"Ran init_weights().\")\n",
    "\n",
    "    def forward(self, data: Data, h_1: Tensor=None, c_1: Tensor=None, \n",
    "                h_2: Tensor=None, c_2: Tensor=None, day_idx: int=0):\n",
    "        # Get data from snapshot\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        \"\"\"\n",
    "        x is [10, 3]\n",
    "        edge_index is [2, num_edges]\n",
    "        edge_attr is [num_edges, 1]\n",
    "        last_day_flag is False unless day graph being passed is last day graph in the time window\n",
    "        \"\"\"\n",
    "        \n",
    "        graphsage_outputs = []\n",
    "        self.concat_feat_list.append(x[:,1:2])\n",
    "\n",
    "        # First GNN Layer\n",
    "        x = self.sage1(x, edge_index, edge_attr)\n",
    "        x = self.graph_norm_1(x)\n",
    "        x = F.relu(x)  \n",
    "        graphsage_outputs.append(x)\n",
    "        \n",
    "        x = self.sage2(x, edge_index, edge_attr)\n",
    "        x = self.graph_norm_2(x)\n",
    "        x = F.relu(x)\n",
    "        graphsage_outputs.append(x)\n",
    "\n",
    "        x = torch.cat(graphsage_outputs, dim=1)\n",
    "\n",
    "        # Initialize hidden and cell states if None\n",
    "        if h_1 is None:\n",
    "            h_1 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "        if c_1 is None:\n",
    "            c_1 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "        if h_2 is None:\n",
    "            h_2 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "        if c_2 is None:\n",
    "            c_2 = torch.zeros(x.shape[0], self.emb_dim)\n",
    "\n",
    "        # RNN Layer\n",
    "        h_1, c_1 = self.lstm1(x, (h_1, c_1))  # h_1 and c_1 both become [10, self.emb_dim]\n",
    "        h_2, c_2 = self.lstm2(h_1, (h_2, c_2))  # h_2 and c_2 both become [10, self.emb_dim]\n",
    "        \n",
    "        if day_idx == self.window_size - 1:\n",
    "            # Skip connection\n",
    "            concat_feat = torch.cat(self.concat_feat_list, dim=1)  # becomes [10, 16]\n",
    "            x = torch.cat((concat_feat, h_1, h_2), dim=1)  # x becomes [10, 16 + 2 * self.emb_dim]\n",
    "            self.concat_feat_list.clear()\n",
    "\n",
    "            # Readout and activation layers\n",
    "            x = self.act1(x)\n",
    "            x = self.lin1(x)\n",
    "            x = self.act2(x)\n",
    "            x = self.lin2(x)\n",
    "\n",
    "        return x, h_1, c_1, h_2, c_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DynamicAdjSAGE(node_features=2, \n",
    "        emb_dim=10, \n",
    "        window_size=WINDOW_SIZE, \n",
    "        output=1, \n",
    "        training=False, \n",
    "        lstm_type=\"vanilla\", \n",
    "        name=\"DASAGE\")\n",
    "\n",
    "# If not running on Syed's laptop, then need to change this path to directory where 100 DCSAGE 14-day models are stored\n",
    "checkpoint = torch.load(\"/Users/syedrizvi/Desktop/Projects/GNN_Project/DCSAGE/Training-Code/training-runs-multiple-models/\" + TRAINING_RUN + \"/model_\" + str(MODEL_IDX) + \".pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.sage1.lin_l.weight.shape)  # lin_l is weights for aggregated neighbor features\n",
    "print(model.sage1.lin_r.weight.shape)  # lin_r is weights for node's own features\n",
    "print(model.sage2.lin_l.weight.shape)  # lin_l is weights for aggregated neighbor features\n",
    "print(model.sage2.lin_r.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage1_lin_l = model.sage1.lin_l.weight.detach().numpy()\n",
    "sage1_lin_r = model.sage1.lin_r.weight.detach().numpy()\n",
    "sage2_lin_l = model.sage2.lin_l.weight.detach().numpy()\n",
    "sage2_lin_r = model.sage2.lin_r.weight.detach().numpy()\n",
    "\n",
    "print(sage1_lin_l.shape)\n",
    "print(sage1_lin_r.shape)\n",
    "print(sage2_lin_l.shape)\n",
    "print(sage2_lin_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.lstm1.weight_hh.shape)\n",
    "print(model.lstm1.weight_ih.shape)\n",
    "print(model.lstm2.weight_hh.shape)\n",
    "print(model.lstm2.weight_ih.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1_hh = model.lstm1.weight_hh.detach().numpy()\n",
    "lstm1_ih = model.lstm1.weight_ih.detach().numpy()\n",
    "lstm2_hh = model.lstm2.weight_hh.detach().numpy()\n",
    "lstm2_ih = model.lstm2.weight_ih.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Extracted Weights as Heatmaps, Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights_heatmap(model_weights, title, save_name):\n",
    "    mean = model_weights.mean()\n",
    "    std = model_weights.std()\n",
    "    median = np.median(model_weights)\n",
    "\n",
    "    sns.heatmap(model_weights)\n",
    "    plt.title(\"Model {} {} \\n(Mean: {:.4f}, Median: {:.4f}, STD: {:.4f})\".format(MODEL_IDX, title, mean, median, std))\n",
    "    plt.savefig(\"model{}_{}\".format(MODEL_IDX, save_name), bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights_heatmap(sage1_lin_l, title=\"GraphSAGE Layer 1 Neighbor Aggr Weights\", save_name=\"sage_1_neighbor_weights_heatmap\")\n",
    "visualize_weights_heatmap(sage1_lin_r, title=\"GraphSAGE Layer 1 Self Aggr Weights\", save_name=\"sage_1_self_weights_heatmap\")\n",
    "visualize_weights_heatmap(sage2_lin_l, title=\"GraphSAGE Layer 2 Neighbor Aggr Weights\", save_name=\"sage_2_neighbor_weights_heatmap\")\n",
    "visualize_weights_heatmap(sage2_lin_r, title=\"GraphSAGE Layer 2 Self Aggr Weights\", save_name=\"sage_2_self_weights_heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights_heatmap(lstm1_hh, title=\"LSTM Cell 1 hh\", save_name=\"lstm_1_hh_heatmap\")\n",
    "visualize_weights_heatmap(lstm1_ih, title=\"LSTM Cell 1 ih\", save_name=\"lstm_1_ih_heatmap\")\n",
    "visualize_weights_heatmap(lstm2_hh, title=\"LSTM Cell 2 hh\", save_name=\"lstm_2_hh_heatmap\")\n",
    "visualize_weights_heatmap(lstm2_ih, title=\"LSTM Cell 2 ih\", save_name=\"lstm_2_ih_heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights_distribution(model_weights, title, save_name):\n",
    "    mean = model_weights.mean()\n",
    "    std = model_weights.std()\n",
    "    median = np.median(model_weights)\n",
    "\n",
    "    sns.histplot(model_weights.flatten(), kde=True)\n",
    "    plt.title(\"Model {} {} \\n(Mean: {:.4f}, Median: {:.4f},, STD: {:.4f})\".format(MODEL_IDX, title, mean, median, std))\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.savefig(\"model{}_{}\".format(MODEL_IDX, save_name), bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights_distribution(sage1_lin_l, title=\"GraphSAGE Layer 1 Neighbor Aggr Weights\", save_name=\"sage_1_neighbor_weights_distrib\")\n",
    "visualize_weights_distribution(sage1_lin_r, title=\"GraphSAGE Layer 1 Self Aggr Weights\", save_name=\"sage_1_self_weights_distrib\")\n",
    "visualize_weights_distribution(sage2_lin_l, title=\"GraphSAGE Layer 2 Neighbor Aggr Weights\", save_name=\"sage_2_neighbor_weights_distrib\")\n",
    "visualize_weights_distribution(sage2_lin_r, title=\"GraphSAGE Layer 2 Self Aggr Weights\", save_name=\"sage_2_self_weights_distrib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights_distribution(lstm1_hh, title=\"LSTM Cell 1 hh\", save_name=\"lstm_1_hh_distrib\")\n",
    "visualize_weights_distribution(lstm1_ih, title=\"LSTM Cell 1 ih\", save_name=\"lstm_1_ih_distrib\")\n",
    "visualize_weights_distribution(lstm2_hh, title=\"LSTM Cell 2 hh\", save_name=\"lstm_2_hh_distrib\")\n",
    "visualize_weights_distribution(lstm2_ih, title=\"LSTM Cell 2 ih\", save_name=\"lstm_2_ih_distrib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43c3ec5cb0d81e7b9f9908a53ca28aa4318265e5d52f388cac911a9765dd2a07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('gnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
